{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658d083",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0251115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chigozie\\miniconda3\\envs\\zindi\\Lib\\site-packages\\codecarbon\\core\\gpu.py:4: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml\n",
      "[codecarbon WARNING @ 01:01:13] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 01:01:13] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 01:01:13] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 01:01:16] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 01:01:16] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz\n",
      "[codecarbon WARNING @ 01:01:16] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 01:01:16] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 01:01:16] No GPU found.\n",
      "[codecarbon INFO @ 01:01:16] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: Unspecified\n",
      "            \n",
      "[codecarbon INFO @ 01:01:16] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 01:01:16]   Platform system: Windows-11-10.0.22631-SP0\n",
      "[codecarbon INFO @ 01:01:16]   Python version: 3.13.5\n",
      "[codecarbon INFO @ 01:01:16]   CodeCarbon version: 3.0.5\n",
      "[codecarbon INFO @ 01:01:16]   Available RAM : 7.900 GB\n",
      "[codecarbon INFO @ 01:01:16]   CPU count: 4 thread(s) in 4 physical CPU(s)\n",
      "[codecarbon INFO @ 01:01:16]   CPU model: Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 01:01:16]   GPU count: None\n",
      "[codecarbon INFO @ 01:01:16]   GPU model: None\n",
      "[codecarbon INFO @ 01:01:20] Emissions data (if any) will be saved to file c:\\Users\\Chigozie\\competitions\\zindi\\MPEG_Microbiome\\data_main\\mpeg_submission\\pos13_mpeg_final_submission\\emissions.csv\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "tracker = EmissionsTracker(project_name=\"federated_learning_model_emissions2\")\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebb3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os, re, gc\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from typing import Literal, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy, logging\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from modelling_utils import FeatureScaler, normalise_counts, LogisticRegression, PLSLatentTransformer\n",
    "from modelling_utils import test_loop, train_loop, print_info, set_seed\n",
    "from fl_utils import Client, Server, PreprocessClientData, create_dataloader, ClientDataset\n",
    "import visual_utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e07405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4361429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f14951",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157a5e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(base_dir): \n",
    "    \"\"\"\n",
    "    Loads dataset\n",
    "\n",
    "    :param base_dir: Base directory where client data are stored\n",
    "    :returns pd.DataFrame. All dataset in folder path (in kmers x sampleIDs)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f'Loading Dataset from {base_dir} directory')\n",
    "        files = glob(f'{base_dir}/**/*.parquet')\n",
    "        all_data = []\n",
    "        if files:\n",
    "            clients = list(map(lambda x: re.search('Mouth|Nasal|Stool|Skin', x, flags=re.IGNORECASE), files))\n",
    "            clients = [client.group() if client else None for client in clients]\n",
    "        \n",
    "        for _, file in enumerate(files):\n",
    "            all_data.append(pd.read_parquet(file))\n",
    "    except Exception as err:\n",
    "        logger.exception(f'Error loading datasets.\\n{err}')\n",
    "    all_data = pd.concat(all_data, axis=1)\n",
    "    logger.info('Datasets loaded')\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38e301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalModel:\n",
    "    def __init__(self, model_fn):\n",
    "        self.model_fn = model_fn\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        if self.model_fn.global_scaler:\n",
    "            self.scaler = self.model_fn.global_scaler\n",
    "        self.scaler.fit(X)\n",
    "        self.model = self.model_fn.global_model.to(device)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.scaler:\n",
    "            X = self.scaler.transform(X)\n",
    "        X = create_dataloader(X, batch_size=128, shuffle=False)\n",
    "        return X\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.model is None:\n",
    "            self.fit(X)\n",
    "        X_loader = self.transform(X)\n",
    "        \n",
    "        self.model.eval()\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in X_loader:\n",
    "                outputs = self.model(inputs.to(device))\n",
    "                prob = F.softmax(outputs, dim=1)\n",
    "                probs.append(prob)\n",
    "        probs = torch.cat(probs).cpu().numpy()\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cc058",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde10d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('../data/Train.csv')\n",
    "subjects = train_labels.groupby('SubjectID').size().sort_values(ascending=False).index\n",
    "subject_ids = {k:j for k, j in zip(train_labels.filename.str.replace('.mgb', '').str.strip(), train_labels.SubjectID)}\n",
    "subject_info = train_labels[['filename', 'SampleType', 'SubjectID']]\n",
    "subject_info.loc[:, 'filename'] = subject_info.loc[:, 'filename'].str.replace('.mgb', '').str.strip()\n",
    "subject_info =  subject_info.rename(columns={'filename': 'ID', 'SampleType': 'label', 'SubjectID': 'subject_id'}).set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a2f8f",
   "metadata": {},
   "source": [
    "__Loading client data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset from data/fl_data/ directory\n",
      "[codecarbon INFO @ 01:01:35] Energy consumed for RAM : 0.000042 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:01:35] Delta energy consumed for CPU with constant : 0.000126 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:01:35] Energy consumed for All CPU : 0.000126 kWh\n",
      "[codecarbon INFO @ 01:01:35] 0.000167 kWh of electricity used since the beginning.\n",
      "Datasets loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "client_data = load_datasets('../data/fl_data/')\n",
    "print()\n",
    "client_data.shape\n",
    "\n",
    "\n",
    "# transpose to ID vs kmer\n",
    "client_data = client_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de2b9d",
   "metadata": {},
   "source": [
    "__Load client datasets and extract their embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbfbc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AAAAAAAA', 'AAAAAAAC', 'AAAAAAAG'], dtype='object', name='kmer')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers = client_data.columns\n",
    "kmers[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder model\n",
    "pls_scaler = joblib.load('../data/models/pls_scaler.pkl')\n",
    "selected_features = pd.read_csv(glob('../data/select*_canon*_kmers.csv')[0], index_col=0).index\n",
    "selected_features_idx = kmers.get_indexer(selected_features).tolist()\n",
    "\n",
    "pls_model = joblib.load(glob('../data/models/canon*pls_model.pkl')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4b6ec",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Prepare client data by \n",
    "\n",
    "- Splitting data into 4 clients based on subject ID, where a subject is assigned to only one client to maintain privacy\n",
    "\n",
    "- Extracting and scaling autoencoder embeddings for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c631589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data into 4 clients...\n",
      "Normalising and scaling kmer counts...\n",
      "[codecarbon INFO @ 01:01:50] Energy consumed for RAM : 0.000084 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:01:50] Delta energy consumed for CPU with constant : 0.000129 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:01:50] Energy consumed for All CPU : 0.000254 kWh\n",
      "[codecarbon INFO @ 01:01:50] 0.000338 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# prepare Client Data and extract their emebddings\n",
    "prep = PreprocessClientData(pls_scaler, client_data, subject_info)\n",
    "client_splits = prep.split_data(label_id='label', unique_id='subject_id', \n",
    "                                num_clients=4, random_state=42, test_fraction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59babcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(pls_model, X, cols=None):\n",
    "    if cols is not None:\n",
    "        if isinstance(X, np.ndarray):\n",
    "            if all(isinstance(x, int) for x in cols):\n",
    "                X_trans = pls_model.transform(X[:, cols])\n",
    "            else:\n",
    "                raise ValueError(\"For numpy arrays, 'cols' must be a list of integers.\")\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            if all(isinstance(x, int) for x in cols):\n",
    "                X_trans = pls_model.transform(X.iloc[:, cols])\n",
    "            elif all(isinstance(x, str) for x in cols):\n",
    "                X_trans = pls_model.transform(X.loc[:, cols])\n",
    "            else:\n",
    "                raise ValueError(\"For DataFrames, 'cols' must be all integers or all strings.\")\n",
    "        else:\n",
    "            raise TypeError(\"X must be a numpy array or pandas DataFrame.\")\n",
    "    else:\n",
    "        X_trans = pls_model.transform(X)\n",
    "\n",
    "    return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af92f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_client_embeddings(client_data_splits, cols=None):\n",
    "    \"\"\"\n",
    "    Extract pls features\n",
    "\n",
    "    :param client_data_split: Dictionary. Client ID key with their respective datasets\n",
    "    :param cols: Columns used for dimensionality reduction if not all features were used\n",
    "    :returns: Dict[list] of client embeddings for train and validation sets\n",
    "    \"\"\"\n",
    "    logger.info('Extracting client data embeddings....')\n",
    "    client_embeddings = copy.copy(client_splits)\n",
    "\n",
    "    for cid, val in client_splits.items():\n",
    "        if val[1] is not None:\n",
    "            train_test = [get_embeddings(pls_model, x, cols) for x in val[:2]]\n",
    "            client_embeddings[cid][:2] = train_test\n",
    "        else:\n",
    "            client_embeddings[cid][0] = get_embeddings(pls_model, val[0], cols)\n",
    "    return client_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68673d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting client data embeddings....\n"
     ]
    }
   ],
   "source": [
    "client_embeddings = extract_client_embeddings(client_splits, selected_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1eba527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get input shape\n",
    "input_dim = client_embeddings['0'][0].shape[1]\n",
    "num_classes = 4\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9d675",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "- Provide configuration setting for both client and server (where the global model's weights are updated via federated averaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7109b0",
   "metadata": {},
   "source": [
    "__Configuration settings for both client and server__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3123702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration setings for client and server (global model)\n",
    "class ClientConfig:\n",
    "    config = {\n",
    "        'local_epochs' : 5, #number of local training rounds\n",
    "        'loss_fn' : nn.CrossEntropyLoss(),\n",
    "        'optimiser' : optim.SGD, # optim.AdamW,\n",
    "        'lr' : 1e-1, \n",
    "        'weight_decay' : 0,\n",
    "        'random_state' : 42,\n",
    "        'n_epoch_print' : 250,\n",
    "        'verbose' : False,\n",
    "        'validation_fraction': None, # fraction for validation\n",
    "        'batch_size' : 128,\n",
    "        'shuffle' : True # shuffle during training\n",
    "    }\n",
    "\n",
    "class StrategyConfig:\n",
    "    config = {\n",
    "        'fit_fraction' : 1., # train using 100% of the clients\n",
    "        'num_rounds' : 200, # number of training rounds to update global model's weights\n",
    "        'fraction_evaluate': 1., # evaluate on test data of all clients\n",
    "        'early_stopping': True,\n",
    "        'patience' : 50, \n",
    "        'verbose' : True,\n",
    "        'eval_metric' : 'loss',\n",
    "        'random_state':42,\n",
    "        'n_epoch_print': 20\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9ae58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate client and server class\n",
    "client_config = ClientConfig.config\n",
    "strategy_config = StrategyConfig.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb54ac",
   "metadata": {},
   "source": [
    "__Instantiating global model__\n",
    "\n",
    "- Assigning clients with global model parameters and their respective client configuratons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb4c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate global model and clients\n",
    "global_model = LogisticRegression(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# instantiate clients models\n",
    "clients = [Client(global_model, client_embeddings.get(key), config=client_config) for key in client_embeddings.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b3fc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(global_model, clients_models, strategy_config):\n",
    "    \"\"\"\n",
    "    Server App\n",
    "    \n",
    "    :param global_model: Global model class\n",
    "    :param client_models: Client models\n",
    "    :param strategy_config: Confugration settings for server\n",
    "    :returns: Server class object\n",
    "    \"\"\"\n",
    "    server = Server(global_model, clients_models, strategy_config)\n",
    "    config = server.check_server_config(server.strategy_config)\n",
    "    server.train_rounds(**config)\n",
    "    return server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edca6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Federated Learning\n",
      "===============================\n",
      "Total Clients: 4\n",
      "4 clients selected for training and 0 for evaluation\n",
      "\n",
      "Training started in server\n",
      "========================\n",
      "[codecarbon INFO @ 01:02:05] Energy consumed for RAM : 0.000124 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:02:05] Delta energy consumed for CPU with constant : 0.000121 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:02:05] Energy consumed for All CPU : 0.000375 kWh\n",
      "[codecarbon INFO @ 01:02:05] 0.000499 kWh of electricity used since the beginning.\n",
      "Round 1 Train: Loss: 0.31055197638014087, Accuracy: 0.9927611168562565\n",
      "Round 20 Train: Loss: 0.018199650206319664, Accuracy: 0.9986211651154774\n",
      "[codecarbon INFO @ 01:02:20] Energy consumed for RAM : 0.000165 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:02:20] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:02:20] Energy consumed for All CPU : 0.000500 kWh\n",
      "[codecarbon INFO @ 01:02:20] 0.000666 kWh of electricity used since the beginning.\n",
      "Round 40 Train: Loss: 0.012899354221704214, Accuracy: 0.9986211651154774\n",
      "[codecarbon INFO @ 01:02:35] Energy consumed for RAM : 0.000207 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:02:35] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:02:35] Energy consumed for All CPU : 0.000625 kWh\n",
      "[codecarbon INFO @ 01:02:35] 0.000832 kWh of electricity used since the beginning.\n",
      "Round 60 Train: Loss: 0.01110623830564968, Accuracy: 0.9982764563943468\n",
      "Round 80 Train: Loss: 0.01019371945016167, Accuracy: 0.9982764563943468\n",
      "[codecarbon INFO @ 01:02:50] Energy consumed for RAM : 0.000249 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:02:50] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:02:50] Energy consumed for All CPU : 0.000750 kWh\n",
      "[codecarbon INFO @ 01:02:50] 0.000999 kWh of electricity used since the beginning.\n",
      "Round 100 Train: Loss: 0.009637055797097475, Accuracy: 0.9982764563943468\n",
      "[codecarbon INFO @ 01:03:05] Energy consumed for RAM : 0.000290 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:03:05] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:03:05] Energy consumed for All CPU : 0.000875 kWh\n",
      "[codecarbon INFO @ 01:03:05] 0.001166 kWh of electricity used since the beginning.\n",
      "Round 120 Train: Loss: 0.009260017326121906, Accuracy: 0.9982764563943468\n",
      "[codecarbon INFO @ 01:03:20] Energy consumed for RAM : 0.000332 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:03:20] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:03:20] Energy consumed for All CPU : 0.001001 kWh\n",
      "[codecarbon INFO @ 01:03:20] 0.001333 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:03:20] 0.004620 g.CO2eq/s mean an estimation of 145.7009543341957 kg.CO2eq/year\n",
      "Round 140 Train: Loss: 0.008986549651877873, Accuracy: 0.9982764563943468\n",
      "Round 160 Train: Loss: 0.008778421118420185, Accuracy: 0.9982764563943468\n",
      "[codecarbon INFO @ 01:03:35] Energy consumed for RAM : 0.000374 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:03:35] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:03:35] Energy consumed for All CPU : 0.001126 kWh\n",
      "[codecarbon INFO @ 01:03:35] 0.001499 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 01:03:50] Energy consumed for RAM : 0.000415 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:03:50] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:03:50] Energy consumed for All CPU : 0.001251 kWh\n",
      "[codecarbon INFO @ 01:03:50] 0.001666 kWh of electricity used since the beginning.\n",
      "Round 180 Train: Loss: 0.008614270265831652, Accuracy: 0.9982764563943468\n",
      "Round 200 Train: Loss: 0.008481202810324485, Accuracy: 0.9982764563943468\n",
      "\n",
      "Train data: Best epoch: 200, Loss: 0.008481202810324485, Accuracy: 0.9982764563943468\n",
      "\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "# return for use\n",
    "logger.info('Performing Federated Learning\\n===============================')\n",
    "fc_server = server_fn(global_model, clients, strategy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1778fee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mouth': 0, 'Nasal': 1, 'Skin': 2, 'Stool': 3} ['Mouth', 'Nasal', 'Skin', 'Stool']\n"
     ]
    }
   ],
   "source": [
    "# define global model\n",
    "model = GlobalModel(fc_server) \n",
    "\n",
    "# get label IDs and names\n",
    "lab_ids = clients[0].label_ids\n",
    "lab_names = list(lab_ids.keys())\n",
    "\n",
    "print(lab_ids, lab_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising global model's performance on the test data from all clients\n",
    "if clients[0].test_data is not None:\n",
    "    all_test = [(model.test_data, model.test_labels) for model in clients]\n",
    "    X, y = zip(*all_test)\n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    X.shape, y.shape\n",
    "    y = list(map(lambda x: lab_ids.get(x), y))\n",
    "\n",
    "    logger.info('Classification Report')\n",
    "    visual_utils.print_classification_report(model, X, y, lab_names)\n",
    "    print()\n",
    "\n",
    "    logger.info('Classification Evaluation Metrics')\n",
    "    print(visual_utils.classification_eval_metrics(model, X, y))\n",
    "    print()\n",
    "\n",
    "\n",
    "    logger.info('Saving Classification Performance Chart Report')\n",
    "    visual_utils.classification_performance_chart_report(model, X, y, display_names=lab_names)\n",
    "    plt.savefig('classification_performance_chart_report2.png')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae6dff",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40eb14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining Test Predictions for submission\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "logger.info('Obtaining Test Predictions for submission\\n')\n",
    "\n",
    "test_data = pd.read_parquet('../data/test_8kmer.parquet')\n",
    "test_data.shape\n",
    "# transpose to samples x features\n",
    "test_data = test_data.T\n",
    "\n",
    "# get their sample IDs\n",
    "test_idx = test_data.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18cdd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "def prepare_test(X, encoder_model, encoder_scaler, **kwargs):\n",
    "    # normalise and scale using the encoder scaler to get embeddings\n",
    "    norm_X = normalise_counts(X)\n",
    "    test_scaled = encoder_scaler.transform(norm_X)\n",
    "    test_embeddings = get_embeddings(encoder_model, test_scaled, **kwargs)\n",
    "    return test_embeddings\n",
    "\n",
    "\n",
    "def save_file(probs, ids, filename):\n",
    "    cols = lab_names\n",
    "    path = 'preds'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    df = pd.DataFrame()\n",
    "    df['ID'] = test_idx\n",
    "    df[cols] = probs\n",
    "    print(df)\n",
    "    filepath = os.path.join(path, f'{filename}.csv')\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b501d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing test embeddings\n",
      "[codecarbon INFO @ 01:04:05] Energy consumed for RAM : 0.000457 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:04:05] Delta energy consumed for CPU with constant : 0.000125 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:04:05] Energy consumed for All CPU : 0.001376 kWh\n",
      "[codecarbon INFO @ 01:04:05] 0.001833 kWh of electricity used since the beginning.\n"
     ]
    }
   ],
   "source": [
    "# get test embeddings\n",
    "logger.info('Preparing test embeddings')\n",
    "test_embs = prepare_test(test_data, pls_model, pls_scaler, cols=selected_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d7fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test probabilities\n"
     ]
    }
   ],
   "source": [
    "# test probabilities\n",
    "logger.info('Predicting test probabilities')\n",
    "test_probs = model.predict_proba(test_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69922710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving test predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID     Mouth     Nasal      Skin     Stool\n",
      "0     ID_UOIPKJ  0.000026  0.000010  0.000032  0.999931\n",
      "1     ID_XHBQPF  0.994854  0.000312  0.003335  0.001499\n",
      "2     ID_KYILXT  0.996740  0.000154  0.002301  0.000805\n",
      "3     ID_UFGHMX  0.000115  0.000192  0.000233  0.999460\n",
      "4     ID_URMZQG  0.000020  0.999897  0.000047  0.000036\n",
      "...         ...       ...       ...       ...       ...\n",
      "1063  ID_FUMEPV  0.001301  0.997365  0.000367  0.000967\n",
      "1064  ID_RWUAEX  0.000165  0.000521  0.000391  0.998922\n",
      "1065  ID_PLZYXW  0.995092  0.004114  0.000223  0.000572\n",
      "1066  ID_TJNQXM  0.000064  0.000020  0.999878  0.000038\n",
      "1067  ID_DSBIZA  0.000025  0.000016  0.000042  0.999916\n",
      "\n",
      "[1068 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# save file\n",
    "logger.info('Saving test predictions')\n",
    "save_file(test_probs, test_idx, 'federated_learnin_preds2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4257a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Time taken : 2.6945 Mins\n",
      "[codecarbon INFO @ 01:04:14] Energy consumed for RAM : 0.000482 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 01:04:14] Delta energy consumed for CPU with constant : 0.000077 kWh, power : 30.0 W\n",
      "[codecarbon INFO @ 01:04:14] Energy consumed for All CPU : 0.001452 kWh\n",
      "[codecarbon INFO @ 01:04:14] 0.001935 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0008061949567610684"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total time\n",
    "end = time.time()\n",
    "mins = (end - start)/60\n",
    "logger.info(f'\\nTotal Time taken : {mins:.4f} Mins')\n",
    "\n",
    "tracker.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
